P8105 Homework 6
================
December 3rd, 2022

## Part 1: Central Park Weather Data

For this problem, we’ll use the 2017 Central Park weather data from the
`rnoaa` package. The code chunk below will download these data.

``` r
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

``` r
crossval_df = weather_df %>% 
  modelr::bootstrap(n = 100) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  group_by(term) %>% 
  summarize(boot_se = sd(estimate))
```

## Part 2: Washington Post Homicide Data

### Load and clean data

The Washington Post has gathered data on homicides in 50 large U.S.
cities. Let’s load the raw homicide data and inspect it.

``` r
homicide_data = read_csv("data/homicide-data.csv") %>% 
  janitor::clean_names()
homicide_data
```

    ## # A tibble: 52,179 × 12
    ##    uid   repor…¹ victi…² victi…³ victi…⁴ victi…⁵ victi…⁶ city  state   lat   lon
    ##    <chr>   <dbl> <chr>   <chr>   <chr>   <chr>   <chr>   <chr> <chr> <dbl> <dbl>
    ##  1 Alb-…  2.01e7 GARCIA  JUAN    Hispan… 78      Male    Albu… NM     35.1 -107.
    ##  2 Alb-…  2.01e7 MONTOYA CAMERON Hispan… 17      Male    Albu… NM     35.1 -107.
    ##  3 Alb-…  2.01e7 SATTER… VIVIANA White   15      Female  Albu… NM     35.1 -107.
    ##  4 Alb-…  2.01e7 MENDIO… CARLOS  Hispan… 32      Male    Albu… NM     35.1 -107.
    ##  5 Alb-…  2.01e7 MULA    VIVIAN  White   72      Female  Albu… NM     35.1 -107.
    ##  6 Alb-…  2.01e7 BOOK    GERALD… White   91      Female  Albu… NM     35.2 -107.
    ##  7 Alb-…  2.01e7 MALDON… DAVID   Hispan… 52      Male    Albu… NM     35.1 -107.
    ##  8 Alb-…  2.01e7 MALDON… CONNIE  Hispan… 52      Female  Albu… NM     35.1 -107.
    ##  9 Alb-…  2.01e7 MARTIN… GUSTAVO White   56      Male    Albu… NM     35.1 -107.
    ## 10 Alb-…  2.01e7 HERRERA ISRAEL  Hispan… 43      Male    Albu… NM     35.1 -107.
    ## # … with 52,169 more rows, 1 more variable: disposition <chr>, and abbreviated
    ## #   variable names ¹​reported_date, ²​victim_last, ³​victim_first, ⁴​victim_race,
    ## #   ⁵​victim_age, ⁶​victim_sex

There are 52179 observations of 12 variables containing information
about homicides in 50 cities across the US. The data contains the date,
victim information (name, age, sex, race), location (city, state,
latitude, longitude), and status of the case.

Next, let’s tidy the data by creating a `city_state` variable, and
omitting Dallas, Phoenix, Kansas City, and Tulsa, AL (a data entry
error). We will also convert `victim_age` to a numeric variable, and
keep observations only where the victim’s race is White or Black.

``` r
homicide_data = homicide_data %>% 
  mutate(
    state = str_to_upper(state),
    city_state = str_c(city, state, sep = ", "), 
    victim_age = as.numeric(victim_age),
    resolved = as.numeric(disposition == "Closed by arrest")) %>% 
  filter(city_state != "Dallas, TX" & 
           city_state != "Phoenix, AZ" & 
           city_state != "Kansas City, MO" & 
           city_state != "Tulsa, AL", 
         victim_race == "White" | victim_race == "Black")
```

There are now 39693 observations of 14 variables in our cleaned dataset
containing information about homicides in 47 cities across the US.

### Modelling homicide data

Now, using purrr::map, list columns, and unnest, we will run `glm` for
each of the cities in the dataset, and extract the adjusted odds ratio
(and CI) for solving homicides comparing male victims to female victims.

The resulting code creates a dataframe with estimated adjusted ORs and
95% CIs for each city, comparing the odds of solving a homicide among
male victims compared to females.

``` r
OR_df = homicide_data %>% 
  nest(data = -city_state) %>% 
  mutate(models = map(data, ~ glm(resolved ~ victim_age + victim_sex + victim_race, 
                                 data = ., family = "binomial")), 
         results = map(models, broom::tidy)) %>% 
  select(-data, -models) %>% 
  unnest(results) %>% 
  mutate(OR = exp(estimate),
         CIL = exp(estimate - 1.96*std.error),
         CIU = exp(estimate + 1.96*std.error)) %>% 
  filter(term == "victim_sexMale") %>% 
  select(city_state, term, log_OR = estimate, OR, CIL, CIU, p.value)
```

Next, we will plot the estimated ORs and CIs for each city.

``` r
OR_df %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR, colour = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = CIL, ymax = CIU, width = .3)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1), 
        legend.position = "none") + 
  labs(
    x = "Location",
    y = "Adjusted odds ratio",
    title = "Effect of male vs. female victim sex on the odds of solving homicides across major U.S. cities, 2007-2017",
    caption = paste0(
          "Error bars represent the 95% confidence interval.",
          "\nSource: The Washington Post."))
```

![](p8105_hw6_my2731_files/figure-gfm/plot%20ORs-1.png)<!-- -->

We can see that across a majority of cities, the ORs are less than 1,
indicating a decreased odds of resolving a homicide when the victim is
male vs. female. This effect is the greatest for New York, Baton Rouge,
and Omaha. Many cities also have a non-significant association between
victim sex and resolving homicides, indicated by the 95% CI for the
adjusted OR crossing the null value of 1.

On the other hand, there is an increased odds of solving a homicide when
the victim is male vs. female in Albequerque, Stockton, and Fresno.
However, for the 3 cities with the highest adjusted ORs, the 95% CI
crosses the null value of 1, indicating there may be a high variability
in these estimates rather than an effect of victim sex on solving
homicides.

## Part 3: Child Birthweight

Next, we will analyze data gathered to understand the effects of several
variables on a child’s birthweight. This dataset consists of roughly
4000 children and includes variables related to the baby’s sex, age,
body measurements, maternal health, and parents’ socioeconomic
information.

### Load and clean data

The code chunk below loads and clean the data for regression analysis.
We will convert numeric variables into factor variables for categorical
variables `babysex`, `frace`, `malform`, and `mrace`.

``` r
birthweight_df = read_csv("data/birthweight.csv") %>% 
  janitor::clean_names()

birthweight_df = birthweight_df %>% 
  mutate(babysex = as.factor(babysex), 
         frace = as.factor(frace), 
         malform = as.factor(malform), 
         mrace = as.factor(mrace))
```

### Proposing a model for birthweight

First, we want to propose our own model for birthweight based on
variables in the dataset. I will start with selecting underlying factors
that I think predict birthweight. For this, I will include gestational
age, head circumference, length of the baby, mother’s weight at delivery
because I think they are important underlying determinants of
birthweight.

``` r
model_1 = lm(bwt ~ bhead + blength + gaweeks + delwt, data = birthweight_df) 
model_1 %>% broom::tidy() %>% knitr::kable(digits = 4)
```

| term        |   estimate | std.error | statistic | p.value |
|:------------|-----------:|----------:|----------:|--------:|
| (Intercept) | -6249.5265 |   95.3679 |  -65.5307 |       0 |
| bhead       |   135.8153 |    3.5021 |   38.7810 |       0 |
| blength     |    79.1451 |    2.0739 |   38.1619 |       0 |
| gaweeks     |    14.1812 |    1.4916 |    9.5072 |       0 |
| delwt       |     2.0402 |    0.1995 |   10.2281 |       0 |

``` r
model_1 %>% broom::glance() %>% knitr::kable(digits = 4)
```

| r.squared | adj.r.squared |   sigma | statistic | p.value |  df |    logLik |      AIC |      BIC |  deviance | df.residual | nobs |
|----------:|--------------:|--------:|----------:|--------:|----:|----------:|---------:|---------:|----------:|------------:|-----:|
|    0.6954 |        0.6951 | 282.777 |  2475.692 |       0 |   4 | -30667.64 | 61347.27 | 61385.53 | 346798796 |        4337 | 4342 |

Examining the output of `broom::tidy`, we can see coefficient estimates
for each variable in our linear model, and see that the p-value of the
Wald test for each variable is less than 0.05, indicating that they are
all significantly associated with our outcome. Moreover, examining
`broom::glance`, we can see the model fit is decent, with an $R^2$ value
of 0.695, indicating the selected variables explain 69.5% of the
variability in birthweight.

However, we may want to see how certain sociodemographic variables
predict birthweight. Therefore, to start, I will add maternal and
paternal race, cigarettes smoken, family income to the model.

``` r
model_2 = lm(formula = bwt ~ babysex + bhead + blength + delwt + 
               mrace + frace + smoken + fincome, data = birthweight_df)
model_2 %>% broom::glance() %>% knitr::kable(digits = 4)
```

| r.squared | adj.r.squared |    sigma | statistic | p.value |  df |    logLik |      AIC |      BIC |  deviance | df.residual | nobs |
|----------:|--------------:|---------:|----------:|--------:|----:|----------:|---------:|---------:|----------:|------------:|-----:|
|    0.7105 |        0.7096 | 275.9759 |  817.0971 |       0 |  13 | -30557.42 | 61144.84 | 61240.48 | 329632133 |        4328 | 4342 |

``` r
model_2 %>% broom::tidy() %>% knitr::kable(digits = 4)
```

| term        |   estimate | std.error | statistic | p.value |
|:------------|-----------:|----------:|----------:|--------:|
| (Intercept) | -5693.7473 |   98.4694 |  -57.8225 |  0.0000 |
| babysex2    |    35.9129 |    8.5209 |    4.2147 |  0.0000 |
| bhead       |   137.3860 |    3.4158 |   40.2212 |  0.0000 |
| blength     |    78.5530 |    2.0131 |   39.0208 |  0.0000 |
| delwt       |     2.2987 |    0.1981 |   11.6062 |  0.0000 |
| mrace2      |  -155.7588 |   46.5992 |   -3.3425 |  0.0008 |
| mrace3      |   -84.2773 |   72.6949 |   -1.1593 |  0.2464 |
| mrace4      |   -70.4329 |   45.5750 |   -1.5454 |  0.1223 |
| frace2      |     9.5778 |   46.7148 |    0.2050 |  0.8376 |
| frace3      |    23.2409 |   70.1440 |    0.3313 |  0.7404 |
| frace4      |   -55.1552 |   45.2184 |   -1.2197 |  0.2226 |
| frace8      |    -2.4511 |   74.9837 |   -0.0327 |  0.9739 |
| smoken      |    -4.5422 |    0.5933 |   -7.6554 |  0.0000 |
| fincome     |     0.3541 |    0.1766 |    2.0052 |  0.0450 |

Examining `broom::glance`, we can see the model fit has improved, with
an $R^2$ value that indicates the selected variables explain 71.05% of
the variability in birthweight. Examining the output of `broom::tidy`,
however, we can see coefficient estimates for the `frace` variable are
insignificant, and the coefficient for `fincome` is near-zero, with a
p-value of 0.045 that approaches significance. Therefore, we will omit
these two variables.

``` r
model_3 = lm(bwt ~ bhead + blength + gaweeks + delwt + smoken + mrace, data = birthweight_df) 
model_3 %>% broom::glance() %>% knitr::kable(digits = 4)
```

| r.squared | adj.r.squared |    sigma | statistic | p.value |  df |    logLik |      AIC |      BIC |  deviance | df.residual | nobs |
|----------:|--------------:|---------:|----------:|--------:|----:|----------:|---------:|---------:|----------:|------------:|-----:|
|    0.7137 |        0.7131 | 274.3101 |  1349.924 |       0 |   8 | -30533.64 | 61087.28 | 61151.04 | 326041049 |        4333 | 4342 |

``` r
model_3 %>% broom::tidy() %>% knitr::kable(digits = 4)
```

| term        |   estimate | std.error | statistic | p.value |
|:------------|-----------:|----------:|----------:|--------:|
| (Intercept) | -5752.2622 |   97.2921 |  -59.1236 |  0.0000 |
| bhead       |   130.0230 |    3.4247 |   37.9667 |  0.0000 |
| blength     |    75.6670 |    2.0242 |   37.3821 |  0.0000 |
| gaweeks     |    12.3447 |    1.4571 |    8.4723 |  0.0000 |
| delwt       |     2.2828 |    0.1964 |   11.6233 |  0.0000 |
| smoken      |    -4.8472 |    0.5886 |   -8.2352 |  0.0000 |
| mrace2      |  -147.3690 |    9.2591 |  -15.9161 |  0.0000 |
| mrace3      |   -74.8522 |   42.5481 |   -1.7592 |  0.0786 |
| mrace4      |  -117.7264 |   18.7867 |   -6.2665 |  0.0000 |

After removing `frace` and `fincome`, we can see the model fit has
actually improved, with an $R^2$ value that indicates the selected
variables explain 71.37% of the variability in birthweight. The output
of `broom::tidy` shows all variables are significantly associated with
birthweight, when adjusting for the other variables. Notably, we see
`mrace` is a more significant predictor of birthweight, suggesting there
were perhaps too many covariates that lead to multicolinearity in Model
2. Therefore, we will keep `model_3` as our final model.

### Cross-validating models

Now, we want to compare our model to a small model, using length at
birth and gestational age as predictors (main effects only) and a large
model, using head circumference, length, sex, and all interactions
(including the three-way interaction). We will save Model 3 we created
in the previous section as `my_mod` and fit the small and large models.

``` r
my_mod = lm(bwt ~ bhead + blength + gaweeks + delwt + smoken + mrace, data = birthweight_df) 
small_mod = lm(bwt ~ gaweeks + blength, data = birthweight_df)
large_mod = lm(bwt ~ bhead + blength + babysex + bhead*blength*babysex, data = birthweight_df)
```

``` r
cv_df =
  crossv_mc(birthweight_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = cv_df %>% 
  mutate(
    my_mod = map(train, ~lm(bwt ~ bhead + blength + gaweeks + delwt + smoken + mrace, data = .x)),
    small_mod = map(train, ~lm(bwt ~ gaweeks + blength, data = .x)),
    large_mod  = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength*babysex, data = .x))) %>% 
  mutate(
    rmse_my = map2_dbl(my_mod, test, ~rmse(model = .x, data = .y)),
    rmse_small = map2_dbl(small_mod, test, ~rmse(model = .x, data = .y)),
    rmse_large = map2_dbl(large_mod, test, ~rmse(model = .x, data = .y)))
```

``` r
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

![](p8105_hw6_my2731_files/figure-gfm/plot%20rmse-1.png)<!-- -->
