---
title: "P8105 Homework 6"
output: github_document
date: "December 3rd, 2022"
---

```{r setup, include = FALSE}
library(tidyverse)
library(p8105.datasets)
library(modelr)
library(patchwork)

knitr::opts_chunk$set(
  warning = FALSE, 
  message = FALSE,
  fig.dim = c(12, 7))

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Part 1: Central Park Weather Data

For this problem, we’ll use the 2017 Central Park weather data from the `rnoaa` package. The code chunk below will download these data.

```{r load noaa}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Using  5000 bootstrap samples we will produce estimates of $\hat{r}^2$ and $log(\hat{\beta_0}*\hat{\beta_1})$ 

```{r}
weather_bootstrap = weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x)),
    results = map(models, broom::tidy), 
    fits = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results, fits) %>% 
  pivot_wider(
    names_from = term, 
    names_prefix = "b_",
    values_from = estimate
  ) %>% 
  fill(`b_(Intercept)`, .direction = "down") %>% 
  fill(b_tmin, .direction = "up") %>% 
  group_by(.id) %>% 
  summarise_all(funs(sum))
```

I can also use the estimates across bootstrap samples to construct a confidence interval around $\hat{r}^2$ and $log(\hat{\beta_0}*\hat{\beta_1})$. 

```{r}
boot_summary = weather_bootstrap %>% 
  group_by(.id) %>% 
  summarize(log_b0b1 = log(`b_(Intercept)`*b_tmin),
            r_squared = r.squared, 
            cil_log_b0b1 = quantile(log_b0b1, 0.025), 
            ciu_log_b0b1 = quantile(log_b0b1, 0.975), 
            cil_rsquared = quantile(r_squared, 0.025), 
            ciu_rsquared = quantile(r_squared, 0.975)) 

head(boot_summary) %>% knitr::kable(digits = 3)
```

Let's plot these estimates:

```{r}
r_sq_p = boot_summary %>% 
  ggplot(aes(x = r_squared)) + geom_density() + 
  labs(
    x = "R-squared",
    y = "Density",
    title = "Density of R-squared estimates across bootstrapped models")

log_b_p = boot_summary %>% 
  ggplot(aes(x = log_b0b1)) + geom_density() + 
  labs(
    x = "Log(β0*β1)",
    y = "Density",
    title = "Density of log(β0*β1) estimates across bootstrapped models")

r_sq_p + log_b_p
```

The above plots show that the distribution of $\hat{r}^2$ estimates are highest around 1.82, while the highest distribution of $log(\hat{\beta_0}*\hat{\beta_1})$ sits around 3.40.

## Part 2: Washington Post Homicide Data

### Load and clean data

The Washington Post has gathered data on homicides in 50 large U.S. cities. Let's load the raw homicide data and inspect it.

```{r load homicide data}
homicide_data = read_csv("data/homicide-data.csv") %>% 
  janitor::clean_names()

homicide_data
```

There are `r nrow(homicide_data)` observations of `r ncol(homicide_data)` variables containing information about homicides in 50 cities across the US. The data contains the date, victim information (name, age, sex, race), location (city, state, latitude, longitude), and status of the case.

Next, let's tidy the data by creating a `city_state` variable, and omitting Dallas, Phoenix, Kansas City, and Tulsa, AL (a data entry error). We will also convert `victim_age` to a numeric variable, and keep observations only where the victim's race is White or Black.

```{r clean data}
homicide_data = homicide_data %>% 
  mutate(
    state = str_to_upper(state),
    city_state = str_c(city, state, sep = ", "), 
    victim_age = as.numeric(victim_age),
    resolved = as.numeric(disposition == "Closed by arrest")) %>% 
  filter(city_state != "Dallas, TX" & 
           city_state != "Phoenix, AZ" & 
           city_state != "Kansas City, MO" & 
           city_state != "Tulsa, AL", 
         victim_race == "White" | victim_race == "Black")
```

There are now `r nrow(homicide_data)` observations of `r ncol(homicide_data)` variables in our cleaned dataset containing homicide information in 47 cities across the US.

### Modelling homicide data

#### Baltimore, MD

For the city of Baltimore, MD, we will use the `glm` function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. We will save the output as an R object and obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims, keeping all other variables fixed.

```{r baltimore glm}
baltimore_df = homicide_data %>% 
  filter(city_state == "Baltimore, MD") 

baltimore_mod = glm(resolved ~ victim_age + victim_sex + victim_race, 
                    family = "binomial", data = baltimore_df)

baltimore_mod %>% 
  broom::tidy() %>% 
  filter(term == "victim_sexMale") %>% 
   mutate(OR = exp(estimate),
         CIL = exp(estimate - 1.96*std.error),
         CIU = exp(estimate + 1.96*std.error)) %>% 
  select(-statistic, -p.value) %>% 
  rename("Term" = term,
         "Parameter estimate" = estimate, 
         "Standard error" = std.error) %>% 
  knitr::kable(digits = 3)
```

The result of our code shows that in Baltimore, the odds of resolving a homicide when the victim is male is 0.426 times the odds of resolving a homicide when the victim is female. We are 95% confident that the true odds ratio of resolving a homicides lies between 0.325 and 0.558 for male vs. female victims.

#### For all cities

Now, using `purrr::map`, list columns, and `unnest`, we will run `glm` for all cities in the homicide dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. 

The resulting code creates a dataframe with estimated adjusted ORs and 95% CIs for each city, comparing the odds of solving a homicide among male victims compared to females.

```{r map glm}
OR_df = homicide_data %>% 
  nest(data = -city_state) %>% 
  mutate(models = map(data, ~ glm(resolved ~ victim_age + victim_sex + victim_race, 
                                 data = ., family = "binomial")), 
         results = map(models, broom::tidy)) %>% 
  select(-data, -models) %>% 
  unnest(results) %>% 
  mutate(OR = exp(estimate),
         CIL = exp(estimate - 1.96*std.error),
         CIU = exp(estimate + 1.96*std.error)) %>% 
  filter(term == "victim_sexMale") %>% 
  select(city_state, term, log_OR = estimate, OR, CIL, CIU, p.value)
```

Next, we will plot the estimated ORs and CIs for each city.

```{r plot ORs}
OR_df %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR, colour = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = CIL, ymax = CIU, width = .3)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1), 
        legend.position = "none") + 
  labs(
    x = "Location",
    y = "Adjusted odds ratio",
    title = "Effect of male vs. female victim sex on the odds of solving homicides across major U.S. cities, 2007-2017",
    caption = paste0(
          "Error bars represent the 95% confidence interval.",
          "\nSource: The Washington Post."))
```

We can see that across a majority of cities, the ORs are less than 1, indicating a decreased odds of resolving a homicide when the victim is male vs. female. This effect is the greatest for New York, Baton Rouge, and Omaha. Many cities also have a non-significant association between victim sex and resolving homicides, indicated by the 95% CI for the adjusted OR crossing the null value of 1. 

On the other hand, there is an increased odds of solving a homicide when the victim is male vs. female in Albequerque, Stockton, and Fresno. However, for the 3 cities with the highest adjusted ORs, the 95% CI crosses the null value of 1, indicating there may be a high variability in these estimates rather than an effect of victim sex on solving homicides.

## Part 3: Child Birthweight

Next, we will analyze data gathered to understand the effects of several variables on a child’s birthweight. This dataset consists of roughly 4000 children and includes variables related to the baby's sex, age, body measurements, maternal health, and parents' socioeconomic information.

### Load and clean data

The code chunk below loads and clean the data for regression analysis. We will convert `malform` to a logical variable, and convert categorical variables `babysex`, `frace`, and `mrace` into factors, while recoding them.

```{r load birth data}
birthweight_df = read_csv("data/birthweight.csv") %>% 
  janitor::clean_names() %>% 
  mutate(babysex = as.factor(babysex), 
         frace = as.factor(frace), 
         malform = as.logical(malform),
         mrace = as.factor(mrace), 
         babysex = fct_recode(babysex, "male" = "1", "female" = "2"),
         frace = fct_recode(frace, "white" = "1", "black" = "2", "asian" = "3", 
                       "puerto rican" = "4", "other" = "8"),
         mrace = fct_recode(mrace, "white" = "1", "black" = "2", "asian" = "3", 
                       "puerto rican" = "4"))
```

### Proposing a model for birthweight

First, we want to propose our own model for birthweight based on variables in the dataset. I will start with selecting underlying factors that I think predict birthweight. For this, I will include gestational age, head circumference, length of the baby, mother's weight at delivery because I think they are important biological variables which determine birthweight. 

```{r model 1}
model_1 = lm(bwt ~ bhead + blength + gaweeks + delwt, data = birthweight_df) 
summary(model_1) 
```

Examining the `summary(model_1)` output, we can see coefficient estimates for each variable in our linear model, and see that the p-value of the Wald test for each variable is less than 0.05, indicating that they are all significantly associated with our outcome. Moreover, we can see the model fit is decent, with an $R^2$ value of 0.695, indicating the selected variables explain 69.5% of the variability in birthweight. 

However, we may want to see how certain sociodemographic variables predict birthweight. Therefore, to start, I will add maternal and paternal race, cigarettes smoken, family income to the model.

```{r model 2}
model_2 = lm(formula = bwt ~ bhead + blength + delwt + 
               mrace + frace + smoken + fincome, data = birthweight_df)
summary(model_2) 
```

Examining `summary(model_2)` output, we can see the model fit has improved, with an $R^2$ value that indicates the selected variables explain 70.93% of the variability in birthweight. However, we can see coefficient estimates for the `frace` variable are insignificant, and the coefficient for `fincome` is near-zero, with a p-value of 0.045 that approaches significance. Therefore, we will omit these two variables.

```{r model 3}
model_3 = lm(bwt ~ bhead + blength + gaweeks + delwt + smoken + mrace, data = birthweight_df) 
summary(model_3)
```

After removing `frace` and `fincome`, we can see the model fit has actually improved, with an $R^2$ value that indicates the selected variables explain 71.37% of the variability in birthweight. The output of `summary(model_3)` shows all variables are significantly associated with birthweight, when adjusting for the other variables. Notably, we see `mrace` is a more significant predictor of birthweight, suggesting there were perhaps too many covariates that lead to multicolinearity in Model 2. Therefore, we will keep `model_3` as our final model.

Finally, we want to assess model fit and check key assumptions, by plotting the residuals and fitted values using `add_residuals` and `add_predictions`

```{r residual pred plots}
birthweight_df = birthweight_df %>% 
  modelr::add_residuals(model_3) %>% 
  modelr::add_predictions(model_3) 

birthweight_df %>% 
  ggplot(aes(x = pred, y = resid)) + geom_point(alpha = 0.5, size = 0.8, colour = "maroon") + 
  labs(
    title = "Model 3 residuals vs. predicted values",
    x = "Predicted value",
    y = "Residual")

```

The above plot shows that for the most part, our data does not violate assumptions of linearity and does not have unequal error variances, as the points form a 'cloud' shape around the residual = 0 line. We do see, however, a few outliers for some smaller predicted values with large residuals, indicating that we may want to treat or verify these observations for erroneous data entries. 

### Cross-validating models

Now, we want to compare our model to a small model, using length at birth and gestational age as predictors (main effects only) and a large model, using head circumference, length, sex, and all interactions (including the three-way interaction). We will save Model 3 we created in the previous section as `my_mod` and fit the small and large models to the full dataset.

```{r fitting 3 models}
my_mod = lm(bwt ~ bhead + blength + gaweeks + delwt + smoken + mrace, data = birthweight_df) 
small_mod = lm(bwt ~ gaweeks + blength, data = birthweight_df)
large_mod = lm(bwt ~ bhead + blength + babysex + bhead*blength*babysex, data = birthweight_df)
```

To assess prediction accuracy, we will compare cross-validated prediction errors between our 3 candidate models. The code chunk below creates a dataframe sampling training and testing data from `birthweight_df`. We then use `map` to fit each candidate model on each training dataset, and obtain the RMSE fitted on the testing data.

```{r cv}
cv_df =
  crossv_mc(birthweight_df, 1000) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = cv_df %>% 
  mutate(
    my_mod = map(train, ~lm(bwt ~ bhead + blength + gaweeks + delwt + smoken + mrace, data = .x)),
    small_mod = map(train, ~lm(bwt ~ gaweeks + blength, data = .x)),
    large_mod  = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength*babysex, data = .x))) %>% 
  mutate(
    rmse_my = map2_dbl(my_mod, test, ~rmse(model = .x, data = .y)),
    rmse_small = map2_dbl(small_mod, test, ~rmse(model = .x, data = .y)),
    rmse_large = map2_dbl(large_mod, test, ~rmse(model = .x, data = .y)))
```

Finally, we can plot the distribution of RMSE values to assess the prediction accuracy across our 3 candidate models.

```{r plot rmse}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin() + 
  labs(
    title = "RMSE of 3 candidate models predicting child birthweight",
    x = "Model",
    y = "RMSE")
```

The above plot shows that my chosen model has the best prediction accuracy, with a majority of RMSEs ranging around 275. This is followed by the large model, which has the highest frequency of RMSE values around 290, then the small model, which has much higher RMSE values of around 325-350. Therefore, I would choose my model to model child birthweight, as it offers the best prediction accuracy among the 3 candidates, and offers better interpretability compared to the large model by avoiding 3-way interactions.
